{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c0e835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ôªÔ∏è Perubahan terdeteksi. Menggabungkan ulang semua file...\n",
      "‚úÖ File gabungan berhasil dibuat: c:\\laragon\\www\\siger-lampung\\ai-backend\\source-chatbot.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Path\n",
    "txt_dir = os.path.abspath(\"../bahan-chatbot/txt\")\n",
    "output_file = os.path.abspath(\"source-chatbot.txt\")\n",
    "meta_file = os.path.abspath(\"source-chatbot.txt.meta\")\n",
    "\n",
    "# Ambil daftar file .txt (urut biar konsisten)\n",
    "current_files = sorted([f for f in os.listdir(txt_dir) if f.endswith(\".txt\")])\n",
    "\n",
    "# Fungsi baca metadata lama\n",
    "def read_meta():\n",
    "    if os.path.exists(meta_file):\n",
    "        with open(meta_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "# Fungsi simpan metadata baru\n",
    "def save_meta(files_list):\n",
    "    with open(meta_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(files_list, f)\n",
    "\n",
    "# Bandingkan dengan metadata lama\n",
    "old_meta = read_meta()\n",
    "\n",
    "if old_meta == current_files and os.path.exists(output_file):\n",
    "    print(\"‚úÖ Tidak ada perubahan file. Menggunakan source-chatbot.txt yang lama.\")\n",
    "else:\n",
    "    print(\"‚ôªÔ∏è Perubahan terdeteksi. Menggabungkan ulang semua file...\")\n",
    "    combined_text = \"\"\n",
    "    for filename in current_files:\n",
    "        file_path = os.path.join(txt_dir, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            combined_text += f.read().strip() + \"\\n\\n\"\n",
    "    # Simpan file gabungan\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(combined_text.strip())\n",
    "    # Simpan metadata baru\n",
    "    save_meta(current_files)\n",
    "    print(\"‚úÖ File gabungan berhasil dibuat:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70615dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Using cached pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
      "  Using cached pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pdfplumber) (11.3.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Using cached pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20250506->pdfplumber)\n",
      "  Downloading cryptography-45.0.6-cp37-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\m.s.i\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
      "Using cached pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
      "Using cached pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "Downloading cryptography-45.0.6-cp37-abi3-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/3.4 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.5/3.4 MB 699.0 kB/s eta 0:00:05\n",
      "   ------ --------------------------------- 0.5/3.4 MB 699.0 kB/s eta 0:00:05\n",
      "   ------ --------------------------------- 0.5/3.4 MB 699.0 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.8/3.4 MB 559.5 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.8/3.4 MB 559.5 kB/s eta 0:00:05\n",
      "   ------------ --------------------------- 1.0/3.4 MB 592.2 kB/s eta 0:00:04\n",
      "   ------------------ --------------------- 1.6/3.4 MB 777.0 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 1.8/3.4 MB 838.9 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.4/3.4 MB 980.0 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.6/3.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.9/3.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.1/3.4 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.4/3.4 MB 1.1 MB/s  0:00:03\n",
      "Using cached pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "Installing collected packages: pypdfium2, cryptography, pdfminer.six, pdfplumber\n",
      "\n",
      "   ---------------------------------------- 0/4 [pypdfium2]\n",
      "   ---------------------------------------- 0/4 [pypdfium2]\n",
      "   ---------------------------------------- 0/4 [pypdfium2]\n",
      "   ---------- ----------------------------- 1/4 [cryptography]\n",
      "   ---------- ----------------------------- 1/4 [cryptography]\n",
      "   ---------- ----------------------------- 1/4 [cryptography]\n",
      "   ---------- ----------------------------- 1/4 [cryptography]\n",
      "   ---------- ----------------------------- 1/4 [cryptography]\n",
      "   ---------- ----------------------------- 1/4 [cryptography]\n",
      "   -------------------- ------------------- 2/4 [pdfminer.six]\n",
      "   -------------------- ------------------- 2/4 [pdfminer.six]\n",
      "   -------------------- ------------------- 2/4 [pdfminer.six]\n",
      "   -------------------- ------------------- 2/4 [pdfminer.six]\n",
      "   ------------------------------ --------- 3/4 [pdfplumber]\n",
      "   ------------------------------ --------- 3/4 [pdfplumber]\n",
      "   ---------------------------------------- 4/4 [pdfplumber]\n",
      "\n",
      "Successfully installed cryptography-45.0.6 pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b96979a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\M.S.I\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import pdfplumber\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "from IPython.display import display, Markdown  # Untuk Jupyter/Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67f489a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TxtChatbot:\n",
    "    def __init__(self, model):\n",
    "        \"\"\"Inisialisasi Chatbot yang membaca dari file teks.\"\"\"\n",
    "        self.model = model\n",
    "        self.source_text = None\n",
    "        self.data_source_name = None\n",
    "        print(f\"‚úÖ TxtChatbot berhasil diinisialisasi dengan model '{model.model_name}'!\")\n",
    "\n",
    "    def load_from_combined_txt(self, combined_txt_path):\n",
    "        \"\"\"Memuat seluruh teks dari satu file .txt gabungan.\"\"\"\n",
    "        self.data_source_name = os.path.basename(combined_txt_path)\n",
    "        print(f\"üìÇ Membaca sumber data utama dari: '{self.data_source_name}'\")\n",
    "        try:\n",
    "            with open(combined_txt_path, 'r', encoding='utf-8') as f:\n",
    "                self.source_text = f.read()\n",
    "            if not self.source_text.strip():\n",
    "                print(\"‚ö†Ô∏è Peringatan: File sumber data kosong.\")\n",
    "                return False\n",
    "            \n",
    "            print(\"‚úÖ Sumber data berhasil dimuat.\")\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå File sumber data tidak ditemukan. Jalankan proses pembaruan terlebih dahulu.\")\n",
    "            return False\n",
    "\n",
    "    def get_info(self):\n",
    "        \"\"\"Menampilkan statistik dari teks yang dimuat.\"\"\"\n",
    "        if not self.source_text:\n",
    "            print(\"‚ùå Belum ada data yang dimuat.\")\n",
    "            return\n",
    "        lines = self.source_text.count('\\n') + 1\n",
    "        words = len(self.source_text.split())\n",
    "        chars = len(self.source_text)\n",
    "        info = (f\"**üìä INFORMASI SUMBER DATA**\\n\"\n",
    "                f\"- üìÑ **Sumber:** {self.data_source_name}\\n\"\n",
    "                f\"- üìù **Total karakter:** {chars:,}\\n\"\n",
    "                f\"- üó£Ô∏è **Total kata:** {words:,}\\n\"\n",
    "                f\"- üìÑ **Total baris:** {lines:,}\")\n",
    "        try:\n",
    "            display(Markdown(info))\n",
    "        except NameError:\n",
    "            print(info.replace('**', ''))\n",
    "\n",
    "    def chunk_text(self, text, max_length=100000):\n",
    "        \"\"\"Memecah teks menjadi beberapa bagian jika terlalu panjang.\"\"\"\n",
    "        if len(text) <= max_length:\n",
    "            return [text]\n",
    "        \n",
    "        chunks, words = [], text.split()\n",
    "        current_chunk, current_length = [], 0\n",
    "        for word in words:\n",
    "            word_length = len(word) + 1\n",
    "            if current_length + word_length > max_length:\n",
    "                if current_chunk: chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk, current_length = [word], word_length\n",
    "            else:\n",
    "                current_chunk.append(word)\n",
    "                current_length += word_length\n",
    "        if current_chunk: chunks.append(\" \".join(current_chunk))\n",
    "        print(f\"üìù Teks sumber terlalu besar, dibagi menjadi {len(chunks)} bagian untuk dianalisis.\")\n",
    "        return chunks\n",
    "\n",
    "    def get_response(self, user_question):\n",
    "        \"\"\"Menghasilkan jawaban berdasarkan teks yang dimuat.\"\"\"\n",
    "        if not self.source_text:\n",
    "            return \"‚ùå Belum ada data yang dimuat. Harap jalankan `load_from_combined_txt` terlebih dahulu.\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"ü§ñ Memproses pertanyaan: {user_question}\")\n",
    "            chunks = self.chunk_text(self.source_text)\n",
    "            \n",
    "            if len(chunks) == 1:\n",
    "                prompt = f\"\"\"Anda adalah Asisten AI Analis Dokumen yang sangat teliti, yang bertugas memberikan jawaban dengan mengakses informasi dari dokumen yang diberikan.\n",
    "                Aturan utama anda :\n",
    "                1. JAWAB HANYA berdasarkan informasi dari <dokumen> yang diberikan.    \n",
    "                2. JANGAN menambahkan informasi, asumsi, atau pengetahuan eksternal apapun jika tidak diminta.\n",
    "                3. Jawaban harus dalam Bahasa Indonesia yang ringkas dan jelas.\n",
    "                4. Batasi jawaban anda MAKSIMAL 250 kata.\n",
    "                5. Jika informasi tidak ada dalam dokumen jawab dengan : \"Informasi tidak ditemukan dalam sumber yang dimiliki\"\n",
    "\n",
    "<dokumen>\n",
    "{chunks[0]}\n",
    "</dokumen>\n",
    "\n",
    "Pertanyaan: {user_question}\n",
    "\n",
    "Jawaban:\"\"\"\n",
    "                response = self.model.generate_content(prompt)\n",
    "                return response.text if response.parts else \"‚ùå Respons diblokir oleh filter keamanan.\"\n",
    "\n",
    "            else:\n",
    "                all_responses = []\n",
    "                print(f\"üìä Menganalisis {len(chunks)} bagian teks...\")\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    print(f\"‚è≥ Menganalisis bagian {i+1}/{len(chunks)}...\", end='\\r')\n",
    "                    prompt = f\"\"\"Dari bagian dokumen berikut, ekstrak semua informasi yang relevan dengan pertanyaan: \"{user_question}\". Jika tidak ada yang relevan, katakan 'Tidak ada informasi relevan'.\n",
    "\n",
    "<dokumen_bagian>\n",
    "{chunk}\n",
    "</dokumen_bagian>\n",
    "\n",
    "Informasi Relevan:\"\"\"\n",
    "                    response = self.model.generate_content(prompt)\n",
    "                    if response.parts and \"tidak ada informasi relevan\" not in response.text.lower():\n",
    "                        all_responses.append(response.text)\n",
    "                print(\"\\nAnalisis selesai.\")\n",
    "                \n",
    "                if not all_responses:\n",
    "                    return \"‚ùå Informasi yang relevan dengan pertanyaan Anda tidak ditemukan di dalam dokumen.\"\n",
    "                \n",
    "                combined_info = \"\\n\\n---\\n\\n\".join(all_responses)\n",
    "                synthesis_prompt = f\"\"\"Anda adalah asisten AI yang cerdas. Berdasarkan kumpulan informasi berikut yang diekstrak dari berbagai bagian dokumen, jawablah pertanyaan pengguna secara komprehensif.\n",
    "\n",
    "<informasi_terkumpul>\n",
    "{combined_info}\n",
    "</informasi_terkumpul>\n",
    "\n",
    "Pertanyaan Pengguna: {user_question}\n",
    "\n",
    "Jawaban Akhir (dalam Bahasa Indonesia):\"\"\"\n",
    "                final_response = self.model.generate_content(synthesis_prompt)\n",
    "                return final_response.text if final_response.parts else \"‚ùå Gagal menghasilkan jawaban akhir karena diblokir oleh filter keamanan.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Terjadi kesalahan tak terduga: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9937d9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ API Key configured!\n",
      "\n",
      "==================================================\n",
      "‚úÖ TxtChatbot berhasil diinisialisasi dengan model 'models/gemini-2.5-pro'!\n",
      "üìÇ Membaca sumber data utama dari: 'source-chatbot.txt'\n",
      "‚úÖ Sumber data berhasil dimuat.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Konfigurasi Gemini Models\n",
    "    API_KEY = \"AIzaSyAXMr24XVP1ohfCO29GdM-9nm1IpBF_A_o\"\n",
    "    try:\n",
    "        genai.configure(api_key=API_KEY)\n",
    "        print(\"‚úÖ API Key configured!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gagal mengkonfigurasi API Key: {e}\")\n",
    "        exit()\n",
    "\n",
    "    my_generation_config = {\n",
    "        \"temperature\": 0.5,\n",
    "        \"max_output_tokens\": 4096,\n",
    "        \"top_p\": 0.6\n",
    "    }\n",
    "    my_safety_settings = {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    }\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"models/gemini-2.5-pro\",\n",
    "        generation_config=my_generation_config,\n",
    "        safety_settings=my_safety_settings\n",
    "    )\n",
    "\n",
    "    #Inisiasi dan load data ke model\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    chatbot = TxtChatbot(model=model)\n",
    "    success = chatbot.load_from_combined_txt(\"source-chatbot.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "486f2312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**üìä INFORMASI SUMBER DATA**\n",
       "- üìÑ **Sumber:** combined.txt\n",
       "- üìù **Total karakter:** 149,304\n",
       "- üó£Ô∏è **Total kata:** 20,337\n",
       "- üìÑ **Total baris:** 822"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ü§ñ Memproses pertanyaan: apa itu DTSEN?\n",
      "üìù Teks sumber terlalu besar, dibagi menjadi 2 bagian untuk dianalisis.\n",
      "üìä Menganalisis 2 bagian teks...\n",
      "‚è≥ Menganalisis bagian 2/2...\n",
      "Analisis selesai.\n",
      "\n",
      "‚ùì Pertanyaan: apa itu DTSEN?\n",
      "ü§ñ Jawaban:\n",
      "----------------------------------------\n",
      "Berdasarkan kumpulan informasi yang diberikan, berikut adalah jawaban komprehensif mengenai apa itu DTSEN:\n",
      "\n",
      "DTSEN adalah singkatan dari **Data Tunggal Sosial dan Ekonomi Nasional**. Ini merupakan sebuah basis data tunggal yang akurat, terkini, dan terintegrasi untuk individu dan/atau keluarga di Indonesia, yang mencakup informasi kondisi sosial, ekonomi, peringkat kesejahteraan, serta data *by name by address*.\n",
      "\n",
      "Secara lebih rinci, DTSEN dapat dijelaskan melalui beberapa poin utama berikut:\n",
      "\n",
      "**1. Tujuan dan Fungsi Utama**\n",
      "Tujuan utama dari DTSEN adalah untuk mendukung keterpaduan program pembangunan nasional dan memastikan program-program pemerintah, terutama yang berkaitan dengan kesejahteraan sosial, dapat terlaksana secara **tepat sasaran, efektif, efisien, dan akuntabel**.\n",
      "\n",
      "Secara spesifik, DTSEN berfungsi sebagai:\n",
      "*   **Acuan utama** dalam penetapan pemberian bantuan sosial, pemberdayaan sosial, dan program penyelenggaraan kesejahteraan sosial lainnya.\n",
      "*   **Alat untuk menetapkan prioritas** penerima Bantuan Sosial (Bansos) berdasarkan pemeringkatan (desil) status sosial ekonomi keluarga.\n",
      "*   **Fondasi untuk menegakkan keadilan sosial**, dengan memastikan setiap bantuan sampai tepat sasaran, tepat waktu, dan tepat jumlah.\n",
      "\n",
      "**2. Komposisi dan Sumber Data**\n",
      "DTSEN dibentuk dengan menggabungkan dan menyatukan tiga jenis data sosial ekonomi utama, yaitu:\n",
      "*   **REGSOSEK** (Registrasi Sosial Ekonomi)\n",
      "*   **P3KE** (Pensasaran Percepatan Penghapusan Kemiskinan Ekstrem)\n",
      "*   **DTKS** (Data Terpadu Kesejahteraan Sosial)\n",
      "\n",
      "Data gabungan ini kemudian dipadankan dan divalidasi dengan data kependudukan dari Dukcapil, serta diperkaya dengan data administratif dari sumber lain seperti PLN, Pertamina, dan BPJS untuk meningkatkan keakuratannya.\n",
      "\n",
      "**3. Sifat Data, Pengelola, dan Dasar Hukum**\n",
      "*   **Sifat Data:** DTSEN bersifat **dinamis**, artinya data dan peringkat kesejahteraan di dalamnya dapat berubah dan dimutakhirkan secara berkala, yaitu setiap 3 bulan sekali.\n",
      "*   **Pengelola:** **Badan Pusat Statistik (BPS)** adalah lembaga yang bertugas menyusun, mengelola, dan mengintegrasikan data untuk menghasilkan DTSEN. Sementara itu, **Kementerian Sosial (Kemensos)** melakukan sinkronisasi dengan BPS dan menggunakan data ini untuk penyaluran bantuan sosial.\n",
      "*   **Dasar Hukum:** Pembentukan dan penggunaan DTSEN didasarkan pada payung hukum yang kuat, yaitu **Instruksi Presiden (Inpres) Nomor 4 Tahun 2025** dan **Peraturan Menteri Sosial Nomor 3 Tahun 2025**.\n"
     ]
    }
   ],
   "source": [
    "if success:\n",
    "    chatbot.get_info()\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    question = \"apa itu DTSEN?\"\n",
    "    answer = chatbot.get_response(question)\n",
    "    \n",
    "    print(f\"\\n‚ùì Pertanyaan: {question}\")\n",
    "    print(\"ü§ñ Jawaban:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Gagal memuat data. Chatbot tidak dapat digunakan.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
